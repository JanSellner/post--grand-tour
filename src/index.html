<!doctype html>

<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<script src="https://distill.pub/template.v2.js"></script>
<!-- <script src="js/_distill/template.v2.js"></script> -->

  <!-- bootstrap -->
<!-- <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>

<script src="js/lib/webgl_utils/webgl-utils.js"></script>
<script src="js/lib/webgl_utils/initShaders2.js"></script>
<script src="js/lib/webgl_utils/MV.js"></script>
<script src="js/lib/numeric-1.2.6.js"></script>
<script src="js/lib/math.js"></script>
<script src="js/lib/lz-string.js"></script>
<script src="js/lib/d3/v5/d3.js"></script>

<script src="js/modules/utils.js"></script>
<script src="js/modules/constants.js"></script>
<script src="js/modules/GrandTour.js"></script>

<script src="js/modules/TeaserRenderer.js"></script>
<script src="js/modules/TeaserOverlay.js"></script>

<script src="js/modules/SoftmaxComparisonRenderer.js"></script>
<script src="js/modules/SoftmaxComparisonOverlay.js"></script>

<!-- <script src="js/modules/ConfusionMatrixRenderer.js"></script> -->

<script src="js/modules/TesseractRenderer.js"></script>
<script src="js/modules/TesseractOverlay.js"></script>

<!-- <script src="js/modules/NeuralNetRenderer.js"></script> -->
<script src="js/modules/NeuralNetOverlay.js"></script>

<script src="js/modules/SmallMultipleRenderer.js"></script>
<script src="js/modules/SmallMultipleOverlay.js"></script>
<script src="js/sm0.js"></script>

<script src="js/modules/LayerTransitionRenderer.js"></script>
<script src="js/modules/LayerTransitionOverlay.js"></script>

<script src="js/modules/LossHistoryRenderer.js"></script>


<script>
  utils.cacheAll(constants.preloadUrls);
  for (let url of constants.layerTransitionUrls['fashion-mnist']){
    window.requestIdleCallback(()=>{utils.cacheAll([url,] )});
  }
  for (let url of constants.adversarialUrls){
    window.requestIdleCallback(()=>{utils.cacheAll([url,] )});
  }
  
  // window.requestIdleCallback(()=>{utils.cacheAll(constants.adversarialUrls)});
  // window.requestIdleCallback(()=>{utils.cacheAll(constants.layerTransitionUrls['fashion-mnist'])});
  // window.requestIdleCallback(()=>{utils.cacheAll(constants.layerTransitionUrls['mnist'])});
  // window.requestIdleCallback(()=>{utils.cacheAll(constants.layerTransitionUrls['cifar10'])});
  // 

  let allViews = [];// teaser, nn2, sm0, nngt, tesseract, se, dm1, nngt2, lt2, lta];
</script>

<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" integrity="sha384-hWVjflwFxL6sNzntih27bfxkr27PmbbK/iSvJ+a4+0owXq79v+lsFkW54bOGbiDQ" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="css/style.css">

<style>
.nav{
  display: block;
}
</style>
</head>

<body>

<d-front-matter>
<script id='distill-front-matter' type="text/json">{
    "title": "Visualizing neural networks with the Grand Tour",
    "description": "We present a way of visualizing activation vectors of neurons in neural network using a technique called the Grand Tour. The random rotation of high dimensional data points shows how trained network confuses different classes during the training process. In addition, our technique allows users to simply change the view point by mouse dragging.",
    "published": "not published",
    "authors": [
      {
        "author":"Mingwei Li",
        "authorURL":"http://hdc.cs.arizona.edu/~mwli/",
        "affiliation":"University of Arizona",
        "affiliationURL":"https://www.cs.arizona.edu/"
      },
      {
        "author":"Zhenge Zhao",
        "authorURL":"https://zhengezhao.wordpress.com/",
        "affiliation":"University of Arizona",
        "affiliationURL":"https://www.cs.arizona.edu/"
      },
      {
        "author":"Carlos Scheidegger",
        "authorURL":"https://cscheid.net/",
        "affiliation":"University of Arizona",
        "affiliationURL":"https://www.cs.arizona.edu/"
      }
    ],
    "katex": {
      "delimiters": [
        {"left": "$$", "right": "$$", "display": false}
      ]
    }
  }
</script>
</d-front-matter>


<nav class='options navbar navbar-expand-lg navbar-dark bg-dark'>
  <div class="collapse navbar-collapse" id="">
    <ul class="navbar-nav mr-auto">
      
      <li class="nav-item active">
        <a class="nav-link" href="#">Dataset:</a>
      </li>

      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle" href="#" id="dataset-option" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
          mnist
        </a>
        <div class="dropdown-menu bg-dark" aria-labelledby="navbarDropdown">
          <a class="dropdown-item bg-dark" onclick="utils.setDataset('mnist')">mnist</a>
          <a class="dropdown-item bg-dark" onclick="utils.setDataset('fashion-mnist')">fashion-mnist</a>
          <a class="dropdown-item bg-dark" onclick="utils.setDataset('cifar10')">cifar10</a>
        </div>
      </li>
    </ul>
  </div>
</nav>


<!-- teaser -->
<div id="teaser-flex-container" class="flex-container" style="display:flex;">
  <div class="flex-item" style="flex: 1;"></div>
  <d-figure class='teaser flex-item' style=" flex: 3; margin: 0rem 0;">
    <canvas id='teaser' class='grandtour' style="height:280px; flex-grow: 1"></canvas>
  </d-figure>
  <script src='js/teaser.js'></script>
  <div class="flex-item" style="flex: 1;"></div>
</div>
<div class='base-grid'>
<p class='caption' style="grid-column: text;">With the Grand Tour<d-cite key="asimov1985grand"></d-cite>, we learned that 
  <span 
    style="color: #ff7f0e; cursor: pointer;" 
    onmouseover="teaser.overlay.onSelectLegend(1);"
    onmouseout="teaser.overlay.restoreAlpha()">digit 1 
  </span> 
  and 
  <span
    style="color: #7f7f7f; cursor: pointer;"
    onmouseover="teaser.overlay.onSelectLegend(7);"
    onmouseout="teaser.overlay.restoreAlpha()">digit 7
  </span> 
  are classified correctly after training epoch 14 and 21 respectively, later than all the other digits.
</p>
</div>


<script>
  let c = utils.CLEAR_COLOR.map(d=>d*255);
  d3.selectAll('.flex-item')
  .style('background', d3.rgb(...c))
</script>

<d-title>
<!-- 
  grid-template-columns: 
  [screen-start] 1fr 
  [page-start kicker-start] 60px 
  [middle-start] 60px 
  [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px 
  [text-end gutter-start] 60px 
  [middle-end] 60px 
  [page-end gutter-end] 
  1fr 
  [screen-end]; 
-->

<p>
  The Grand Tour<d-cite key="asimov1985grand"></d-cite> is a classic visualization technique for high-dimensional point clouds.
  Although it has fallen out of favor for modern methods such as UMAP <d-cite key="mcinnes2018umap"></d-cite> and t-SNE<d-cite key="maaten2008visualizing"></d-cite>, the Grand Tour offers unique properties in the visualization of the behavior of neural networks.
  Because the Grand Tour is fundamentally a <em>linear</em> method, the patterns it shows more directly correspond to patterns of interest in the neural network.
  We present three use cases of interest: visualizing the training process as the network weights change, visualizing the layer-to-layer behavior as the data travel through the network and visualizing adversarial examples.
  
  <!-- The Grand Tour works by generating a random, smoothly changing rotation of the dataset, and then projecting it to 2D. -->
  <!-- With the Grand Tour, user interaction is easy to incorporate, allowing a natural mixed-initiative mode of visual exploration. -->
  <!-- In the figure above, we use the Grand Tour to display how 1000 images activate the final softmax layer of a 10-class neural network classifier. -->
  <!-- The point color represents the true class of each image. -->
  <!-- Unlike other methods, the Grand Tour offers good consistency with respect to changes in the data, allowing the visualization of training dynamics of a neural network. -->
  <!-- Notice how it is possible to notice how different classes are learned at different epochs of the training process: in the case of MNIST digits, the network learns most digits from the beginning, but it did not -->
  <!-- give a correct answer to most of digit 1 until <a onclick="utils.setTeaser(teaser, 'mnist', 10, [1,])" href="#teaser">epoch 14</a>, or digit 7 until <a onclick="utils.setTeaser(teaser, 'mnist', 17, [7,])" href="#teaser">epoch 21</a>. -->
</p>
</d-title>

<d-article>
<h2>Introduction</h2>
<p>
  Neural networks have outperformed other methods in supervised learning contests such as Imagenet Large Scale Visual Recognition Challenge (ILSVRC)<d-cite key="ILSVRC15"></d-cite>, sometimes performing at the same level as humans in a number of tasks.
  Unfortunately, their decision process is notoriously hard to interpret<d-cite key="lipton2016mythos"></d-cite>, and their training process is often hard to debug<d-cite key="wongsuphasawat2018visualizing"></d-cite>.
  We present a method to visualize the responses of a neural network using an under-appreciated visualization method.
  The <em>Grand Tour</em> works by generating a random, smoothly changing rotation of the dataset, and then projecting the data to the two-dimensional screen. 
  This is a <em>linear</em> process.
  <!-- The Grand Tour works well for visualizing neural networks because, as we will see, neural networks are <em>also</em> (mostly) linear<d-cite key="lee2019wide"></d-cite> transformations from inputs to outputs. -->
  The Grand Tour works well for visualizing neural networks because, as we will see, the linear transformations in neural networks can be simplified in the Grand Tour.
  We will find this data-visual correspondence crucial to the visualization design, especially when compared to other non-linear methods.
</p>
<p>
  To understand a neural network, we often try to observe its action on input examples.
  These kinds of visualizations are useful to elucidate the activation patterns of a neural network for a single example, but they offer little insight about the relationship between different examples, different states of the network as it's being trained, or the same example as it is transformed by the different layers of a single network.
  Therefore, we should instead aim to visualize the <em>context around</em> our examples of interest. 
  This context is precisely the structure that linear methods preserve well.
  To fully understand the neural network process, it is not enough to "open the black box".
  We need a visualization that opens the box, but crucially, does not destroy the patterns we are hoping to find.
  As a concrete example of how visualizations can fail <d-cite key="kindlmann2014algebraic"></d-cite>, if our visualization is not consistent throughout a sequence of training epochs, then changes in the visualization might not be due to any meaningful change in the network itself. 
  Just as importantly, changes in the network might not be reflected in the visualization.
</p>


<h2>Motivation and Background</h2>

<!-- <p>
  There is a number of great videos and blogs that explain what a neural network is (for example the one made by <a href="https://www.youtube.com/watch?v=aircAruvnKk">3Blue1Brown</a>).
  There are also numerous sources explaining <em>different aspects</em> of understanding the internal of a neural network.
  The <d-cite key="smilkov2017direct">Tensorflow Playground</d-cite> by Smilkov and Carter extended the of work of <d-cite key="ConvnetJS">Karpathy</d-cite>. Their visualization benefits from the dimensionality of input being 2, utilizing (x,y) location in a canvas to represent input domain and colors to represent activation levels. If it were more than 2, it would be not as straightforward to visualize in a 2D screen. 
  Nielsen gave <d-cite key="neuralnetcomputeany">a visual proof that neural nets can compute any function</d-cite> in his online book on deep neural networks. In its chapter 4, interactive visualizations are used to visually convey a hard-to-proof concept in deep learning. By letting readers tune the weights in a neural network any visually see the model created, readers convince themselves that a neural network can represent any function. 
  <d-cite key="goh2017why">Goh</d-cite> demonstrate visually why momentum works in optimization algorithms. Because stochastic gradient descent algorithm is the essence of training a neural network, his work gave both an intuition and a mathematical justification of the importance of momentum in the training process.
</p>

<p>
  Despite the various aspects of "understanding a neural network", in this work we will demonstrate a tool for <em>understanding the resultant behavior and performance</em> of neural networks. 
  We first give some necessary background of a neural network. Then we explain the technique we adapt called the Grand Tour, and describe the novel interaction we added onto it for direct user manipulation. 
</p> -->
<!-- <p>
  At a very high level, a neural network is a black box that answers a very specific type of question. 
  Taking a classic example, if we want a machine that helps us read hand written digits, a neural network can do it. 
  The process of automatically tuning a neural network for a specific use (in this case, recognizing digits) is called <em>training</em>. 
  A complete training round during which the network sees all training examples once is often referred to as an <em>epoch</em>.
</p> -->
<!-- <p>
  As you will see in a later demonstration, the trained network may make incorrect claims about some images. This is mostly because the machine only see finite number of training examples (in <em>training dataset</em>) but we test the machine with new, unseen examples. To test how the machine perform on real data, we often prepare a separated <em>testing dataset</em> that machine has never seen in training.
  We could feed our testing examples into the neural network, one at a time, to see if any of them has a wrong answer.
  However, to diagnose the overall performance, we shall not only test with one example at a time. Instead we often look at a <em>summary</em> of many testing examples. 

  Following this principle we often group all testing images by a pair of their properties - (true_class, predicted_class). 
  Presumably the network could perform well on some easy classes but perform bad on some other hard classes. In that's the case we will see interesting counting patterns among different groups. 
  If we count the number of images that falls into different groups, we end up what is called a <em>confusion matrix</em>. 
   
  See the figure below as an example of confusion matrix for our classifier, shown as a heatmap. 
  The more yellow a cell has, the more image examples fall into the corresponding group, i.e. the higher count.
  The color map are consistent through epochs, normalized by maximum of off-diagonal entries from epoch 25 to epoch 99. Under this design, most diagonal entries are likely to saturate because the count of correct predictions is often more than wrong predictions on any off-diagonal cells.
  You can click play button or drag the slider bar handle to see how network performance improved throughout training epochs. Hovering over a cell will show the actual count in the group.
</p> -->

<!-- <d-figure id='confusionMatrix'>
  <script>
    const cmFigure = document.querySelector("d-figure#confusionMatrix");
    let cm;

    cmFigure.addEventListener("ready", function() {
      console.log('cmFigure ready');

      cm = new ConfusionMatrixRenderer('#confusionMatrix');
      let urls = ['data/'+utils.getDataset()+'/bin/confusion.bin'];
      utils.loadDataToRenderer(urls, cm);


      utils.addDatasetListener(function(){
        let urls = ['data/'+utils.getDataset()+'/bin/confusion.bin'];
        delete cm.dataObj;
        cm.isDataReady = false;
        utils.loadDataToRenderer(urls, cm);
      });
      window.addEventListener('resize', ()=>{
        cm.resize();
      });
    });
  </script>
</d-figure> -->


<!-- <p>
  We can see interesting things happening. For example, when <a onclick="utils.setDataset('mnist'); cm.setEpoch(19)" href="#confusionMatrix">looking at epoch 19 of MNIST dataset</a>, we notice that the network incorrectly predicts many digit 7 images to classes of digit 0,1,2,3 and 9. However on epoch 21, those wrong predictions are fixed. 
  Although confusion matrix gives a general summary of how our neural network behaves, we have no means of further investigate the behavior of individual images, because we lose the information about how individual images contributed to this summary. 
  That is why a simple confusion matrix can never answer questions like "which image is hard to predict". We would benefit from a more detailed instance-by-instance look about our network.
</p> -->



<h3>Opening the Black Box</h3>

<p>
  We trained deep CNN models with 3 common image classification datasets: 
  MNIST
  <d-footnote>
    MNIST<d-cite key="lecun2010mnist"></d-cite> contains images of 10 digits
    <img 
    class="img-center" 
    style="width: 70%;" 
    src="figs/mnist.png" />
    Image credit to <a href="https://en.wikipedia.org/wiki/File:MnistExamples.png">https://en.wikipedia.org/wiki/File:MnistExamples.png</a> 
  </d-footnote>,
  fashion-MNIST
  <d-footnote>
    Fashion-MNIST<d-cite key="xiao2017fashion"></d-cite> contains 10 classes of clothes, bags and shoes:
    <img 
    class="img-center" 
    style="width: 70%;" 
    src="figs/fashion-mnist.png" />
    <!-- Image credit to <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>  -->
    Image credit to <a href="https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925">https://towardsdatascience.com/multi-label-classification-and-class-activation-map-on-fashion-mnist-1454f09f5925</a> 

  </d-footnote>

  and CIFAR-10
  <d-footnote>
    CIFAR-10<d-cite key="krizhevsky2009cifar10"></d-cite> contains colored images of 10 classes of objects
    <img 
    class="img-center" 
    style="width: 70%;" 
    src="figs/cifar-10.png" />
    Image credit to <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a> 
  </d-footnote>. 
  Our architecture is simpler in structure than state-of-the-arts but is complex enough to demonstrate the power of the Grand Tour. 
  <!-- Our CNN models are sequential, meaning that the prediction (<d-math>y</d-math>) is derived from input (<d-math>x</d-math>) passing through a sequence of functions, or "layers" in neural network terminology.
  <d-math block>
    y :=  f_L \cdots f_2 \circ f_1(x)
  </d-math> -->
  <!-- For images, the input is not a single scalar but a 2D array of scalar values for gray scale images or RGB triples for colored images, written as <d-math>x \in \mathbb{R}^{w \times h \times c}</d-math>, where <d-math>w</d-math>, <d-math>h</d-math> and <d-math>c</d-math> are width, height and number of channels (<d-math>c = </d-math>1 or 3) respectively.
  When needed, one can always flatten the 2D array into an equivalent <d-math>w \cdot h \cdot c</d-math> -dimensional vector.
  Similarly, the intermediate values after any one of the functions in composition, or activations of neurons after a layer, can also be seen as a vector in <d-math>\mathbb{R}^n</d-math>, where <d-math>n</d-math> is the number of neurons in a layer. 
  This vector view of data in neural network not only allows us represent complex data in a mathematically compact form, but also gives us a way of thinking the design visualizing our data. -->
</p>
<p>
  When training neural networks, we optimize parameters in the function to minimize a scalar-valued loss function.
  We want the loss to keep decreasing, so we monitor the whole history of training and testing losses over rounds of training (or "epochs"), to make sure that the loss decreases over time. 
  Taking the training of an MNIST classifier as an example, the testing loss during training evolves like the following
</p>

<d-figure class='lh' style="">
  <svg id='lh' height=300 style='width: 100%; height: 300px;'></svg>
  <script src='js/lh.js'></script>
</d-figure>

<p>
  Although its general trend meets what we expected, the loss keeps decreasing, we see something weird around epoch 14 and 21: the curve went flat before it dropped again.
  What happened? What caused that?
</p>

<d-figure class='lh-per-class' style="">
  <svg id='lh-per-class' height=300 style='width: 100%; height: 300px;'></svg>
  <script src='js/lh-per-class.js'></script>
</d-figure>

<p>
  If we separate input examples by their true labels/classes and plot the <em>per-class</em> loss like above, we learned that the two drops were caused by the class of digit 1 and 7 - the model learns different classes at very different times in the training process. 
  Although the network learns to recognize digits 0, 2, 3, 4, 5, 6, 8 and 9 early on, it is not until epoch 14 that it starts successfully recognizing digit 1, or until epoch 21 that it recognizes digit 7.
</p>
<p>
  Perhaps the next question is: were these late learning caused by some outliers in the dataset or it is a feature for all 1s or 7s?
  To explore this we need to break the per-class losses further down to individual input examples.
  <!-- TODO smoother transition -->
  But instead of looking at losses, we can directly look at the softmax activations. 
  The softmax layer has 10 neurons that can be seen as the (predicted) probability, one for each class. 
  Together these 10 neurons can also be seen as a 10-vector, which will become useful later.
  We expect, for example, that the 7th (0-indexed) neuron fired up for hand written 7s.
  In fact, since we have access to not only the softmax but also all hidden layers, we can probe the network with input examples to see how it reacts on <em>each</em> layer.
  In this way we are trying to open the "black box" and understand the internal of a neural network.
  Here is a typical view of the opened box:
</p>

<d-figure class='nn2' style="grid-column: page; ">
  <svg id='nn2' height=300 style='width: 100%; height: 300px;'></svg>
  <script src='js/nn2.js'></script>
</d-figure>
<p class='caption'>Neural network opened. The colored blocks are building-block functions (i.e. neural network layers), the gray-scale heatmaps are either the input image or intermediate activation vectors after some layers.</p>
   
<p>
  Even though neural networks are capable of incredible feats of classification, deep down, they really are just pipelines of relatively simple functions.
  In our classifier, the pipeline has convolutional
  <d-footnote>
    A convolution calculates weighted sums of regions in the input. 
    In neural networks, the learnable weights in convolutional layers are referred to as the kernel.
    For example
    <img src="figs/conv.gif" style="width:70%" class="img-center">
    Image credit to <a href="https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9">https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9</a>.<br>
    See also <a href="https://github.com/vdumoulin/conv_arithmetic">Convolution arithmetic</a>.
  </d-footnote> 
   and fully-connected (linear) layers
  <d-footnote>
    A fully-connected layer computes output neurons as weighted sum of input neurons. In matrix form, it is a matrix that linearly transforms the input vector into the output vector.
  </d-footnote>
   , and rectified linear units (ReLU)
  <d-footnote>
    First introduced by Nair and Hinton<d-cite key="nair2010relu"></d-cite>, ReLU calculates <d-math>f(x)=max(0,x)</d-math> for each entry in a vector input. Graphically, it is a hinge at the origin: <img src="figs/relu.png" style="width:60%" class="img-center">
    Image credit to <a href="https://pytorch.org/docs/stable/nn.html#relu">https://pytorch.org/docs/stable/nn.html#relu</a>
  </d-footnote>
  as non-linear layers.
  Passing an input image through this function pipeline, we have it transformed to different activations patterns in hidden layers until the softmax activations
  <d-footnote>
    Softmax function calculates <d-math>S(y_i)=\frac{e^{y_i}}{\Sigma_{j=1}^{N} e^{y_j}}</d-math> for each entry (<d-math>y_i</d-math>) in a vector input (<d-math>y</d-math>). For example, <img src="figs/softmax.png" style="width:75%" class="img-center">
    Image credit to <a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/">https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/</a>
  </d-footnote> 
  in the end.

  For images, the input is a 2D array of scalar values for gray scale images or RGB triples for colored images.
  When needed, one can always flatten the 2D array into an equivalent (<d-math>w \cdot h \cdot c</d-math>) -dimensional vector.
  Similarly, the intermediate value after any one of the functions in composition, or activations of neurons after a layer, can also be seen as a vector in <d-math>\mathbb{R}^n</d-math>, where <d-math>n</d-math> is the number of neurons in the layer. 
  The softmax, for example, can be seen as a 10-vector whose values are positive real numbers that sum up to 1.
  This vector view of data in neural network not only allows us represent complex data in a mathematically compact form, but also hints us on how to visualize them in a better way.

  Most of the simple functions fall into two categories: they are either linear transformations of their inputs (like fully-connected layers or convolutional layers), or relatively simple non-linear functions that work component-wise (like sigmoid activations<d-footnote>
    Sigmoid calculates <d-math>S(x)=\frac{e^{x}}{e^{x}+1}</d-math> for each entry (<d-math>x</d-math>) in a vector input. Graphically, it is an S-shaped curve.
    <img src="figs/sigmoid.png" style="width:60%" class="img-center">
    Image credit to <a href="https://en.wikipedia.org/wiki/Sigmoid_function">https://en.wikipedia.org/wiki/Sigmoid_function</a>
  </d-footnote> 
  or ReLU activations).
  (Some operations, notably max-pooling<d-footnote>
    Max-pooling calculates maximum of a region in the input. For example
    <!-- <img src="figs/maxpool.png" style="width:60%" class="img-center">
    Image credit to <a href="https://computersciencewiki.org/index.php/Max-pooling_/_Pooling">https://computersciencewiki.org/index.php/Max-pooling_/_Pooling</a> -->
    <img src="figs/maxpool.gif" style="width:70%" class="img-center">
    Image credit to <a href="https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9">https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9</a>
  </d-footnote>  and softmax, do not fall into either categories. We will come back to this later.)
  We will later see that this categorization is essential when we try to untangle the complex internal of the neural network.
  Decomposing the whole pipeline into simpler functions gives details about what happened inside, in particular how neurons activate in different layers. 
  However, in the above view we look at examples one at a time, we lose the full picture: what does the network do to <em>all</em> examples?
</p>


<!-- <p>
  TODO NEED THIS LATER?
  In a neural network, it must be the effect of non-linear functions that cause its classification power, because linear methods are fundamentally limited in their predictive ability (CITE VC-DIMENSION).
  However, notice that in neural networks, the non-linearities are in fact easy to understand because they tend to operate component-wise: in effect, one coordinate does not influence another.
  On the other hand, linear layers <em>combine many coordinates</em> in their computation, making the overall behavior hard to understand in terms of the numbers being passed around.
  In linear mappings, the activation of one neuron is potentially distributed to every neuron of the next layer.
  In order to understand the behavior of a linear map, we have to think of all neurons in one layer as a whole.
  In a sense, then, it is the <em>linear</em> parts of a neural network that need most help from visualization.
</p> -->

<p>
Now we want to look at neuron activations (e.g. in the softmax layer) of all examples at once.
Should there be only two neurons/dimensions, a simple scatter plot of these two dimensions would suffice.
However, the points in the softmax layer are 10 dimensional, thus we need to either show two dimensions at a time which does not scale well (as there would be 45 scatter plots - a quadric to the dimensionality - to look at),
or we can use <em>dimensionality reduction</em> to map the data into a two dimensional space and show them in a single plot. 
</p>

<i id="sm0-a"></i>
<h3>The State-of-the-art Dimensionality Reduction is Non-linear</h3>
<p>
  Consider the aforementioned intriguing feature about the different learning rate that the MNIST classifier has on digit 1 and 7: the network did not learn to recognize digit 1 until epoch 14, digit 7 until epoch 21.
  With that in mind, look for changes of digit 1 around epochs 14 and changes of digit 7 around epochs 21 in the visualizations below. 
  The network behavior is not subtle, every digit in those classes is misclassified until the critical epoch. 
  Yet, note how hard to spot it in any of the plots below at first glance. 
  In fact, if inspected carefully, we can see that in the UMAP <d-cite key="mcinnes2018umap"></d-cite> (last row), the digit 1 which cluttered in the bottom in epoch 13 becomes a new tentacle in epoch 14. 
  Use the buttons on the right to identify the change.
</p>
<d-figure id='smallmultiple1' class='smallmultiple' style="">
  <canvas id='sm1' class='smallmultiple' width='100' height='200'></canvas>
  <script>
  let sm1 = createSmallMultiple('#smallmultiple1', 
    [13,14,15, 20,21,22], ['t-SNE', 'Dynamic t-SNE', 'UMAP'], 
    'mnist', true, highlight_digits);
</script>
</d-figure>
<p class='caption'>Softmax activations of the MNIST classifier with non-linear dimensionality reduction</p>

<p>
  The reason that non-linear embeddings fail in spotting the fundamental change in data is that they did not fulfill the principle of <em>data-visual correspondence</em> <d-cite key="kindlmann2014algebraic"></d-cite>:
  we design visualizations to reveal signals in data, so that a change in data is reflected in its visualization correspondingly. 
  Ideally, we further want the changes in two parts (data and its visualization) to <em>match in magnitude</em>: a barely noticeable change in visualization reflects the smallest possible change in data, and a salient change in visualization reflects a significant one in data.
  Here in non-linear embeddings, a significant change happened in only a <em>subset</em> of data (e.g. all points of digit 1 from epoch 13 to 14), but <em>all</em> points in the visualization move dramatically.
  In most non-linear embeddings, the position of each single point depends on the whole data distribution in such embedding algorithms.
  This property is not ideal for visualization because it fails the data-visual correspondence. 
  The lack of data-visual correspondence make it hard to <em>infer</em> the underlying change in data from the visualization.
</p>

<p>
  Non-linear embeddings that have non-convex objectives also tend to be sensitive to initial conditions.
  For example, in MNIST, although the neural network starts to stabilize on epoch 30, t-SNE and UMAP still generate quite different projections between epochs 30 and 99.
  But even with spatial regularization (e.g. Dynamic t-SNE<d-cite key="rauber2016visualizing"></d-cite>), these non-linear methods suffer from interpretability issues<d-cite key="wattenberg2016use"></d-cite>. 
</p>

<d-figure id='smallmultiple2' class='smallmultiple' style="">
  <canvas id='sm2' class='smallmultiple' width='100' height='200'></canvas>
  <script>let sm2 = createSmallMultiple('#smallmultiple2', [1,5,10,15, 20,30,31,32,33], ['t-SNE', 'Dynamic t-SNE', 'UMAP', 'Linear'], 'mnist', true, ()=>{})</script>
</d-figure>

<p>
  As another example, in fashion-MNIST, take the network's three-way confusion among sandals, sneakers and ankle boots.
  You can see it most directly in the linear projection (last row, which we found using the Grand Tour and direct manipulation), as the points form a triangle. 
  The t-SNE, in contrast, incorrectly separates the class clusters (possibly because of an inappropriately-chosen hyperparameter).
  Although UMAP isolates the three classes successfully, the fundamental complexity of non-linear methods makes it hard to know whether this is a feature of data or an artifact in the embedding algorithm.
  If, as in these cases, a fundamental change in a visualization is not only due to the data (e.g. it is also partially due to the embedding algorithm), this bad design can obscure the data interpretation: we may miss important patterns from reading a bad visualization.
</p>

<d-figure id='smallmultiple3' class='smallmultiple' style="">
  <canvas id='sm3' class='smallmultiple' width='100' height='200'></canvas>
  <script>
  let sm3 = createSmallMultiple('#smallmultiple3', 
[2,5,10,20,30,60,99], ['t-SNE', 'UMAP', 'Linear'], 
'fashion-mnist', true, highlight_shoes)</script>
</d-figure>
<p class='caption'>Three-way confusion in fashion-MNIST</p>

<!-- <p> -->
<!--    This can be seen below, where we compare a number of other DR methods: <d-cite key="maaten2008visualizing">t-SNE</d-cite>, <d-cite key="mcinnes2018umap">UMAP</d-cite>, <d-cite key="rauber2016visualizing">Dynamic t-SNE</d-cite> <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA), random linear projections, and a manually-selected linear projection by our direct manipulation in Grand Tour. -->
<!-- </p> -->

<!-- The small multiples are linked by brush so that one can filter points and link them to the same points in other views<i id="sm0-a"></i> -->
  <!-- As emphasized in the abstract, in MNIST we observed different class difficulties.  -->
  <!-- The network learned most digits from the beginning, but it did not -->
  <!-- give correct answers to digit 1 until  -->
  <!-- <\!-- TODO: optmized code -\-> -->
  <!-- <a onclick="utils.setTeaser(nngt, 'mnist', 10, [1,])" href="#nngt-a"> -->
  <!--   epoch 14 -->
  <!-- </a>,  -->
  <!-- or digit 7 until  -->
  <!-- <a onclick="utils.setTeaser(nngt, 'mnist', 17, [7,])" href="#nngt-a"> -->
  <!--   epoch 21 -->
  <!-- </a>. -->
  <!-- This visual clue about training dynamics is somewhat unique to the Grand Tour comparing to other dimensionality techniques. -->

<!--   We plot the final softmax layer with a number of other dimensionality reduction methods: <d-cite key="maaten2008visualizing">t-SNE</d-cite>, <d-cite key="mcinnes2018umap">UMAP</d-cite>, <d-cite key="rauber2016visualizing">Dynamic t-SNE</d-cite> <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA), random linear projections, and a manually-selected linear projection by our direct manipulation in Grand Tour. The small multiples are linked by brush so that one can filter points and link them to the same points in other views<i id="sm0-a"></i>. -->
<!-- </p> -->

<!-- <p>
  In addition, we use this section to highlight a current shortcoming of our present technique, namely that it only allows the visualization of a <em>single</em> projection.
  With multiple plots, we can use <em>linked brushing and multiple views</em>.
  This technique allows users to build an intuitive understanding of the projections by selecting points in one region of a plot and observing which points are highlighted on the other projections.
  </p> -->





<h3>Linear Methods to the Rescue</h3>

<p>
  When given the option, then, we should prefer methods for which changes in the data produce predictable, visually salient changes in the result.
  Linear dimensionality reductions often have this property. 
  They are more interpretable than non-linear methods due to a good data-visual correspondence.
  Principle Component Analysis (PCA) is often the first thing to try when doing linear dimensionality reduction. 
  It works by choosing directions that preserve the most variance in data.
  However, the distribution of data in softmax layers, if trained almost perfectly, has almost <em>equal</em> variance in every axis directions because along each axis there concentrates similar number of examples around the one-hot vector<d-footnote>We are assuming a class-balanced training dataset, otherwise PCA will prefer dimensions with more examples, which might not help much in making sense of the data either. </d-footnote>.
  As a result, even though PCA projections are interpretable and consistent through training epochs, the first two principal components of softmax activations are not substantially better than the third. 
  So which of them should we choose? 
  <!-- To make matters worse, different principal components are orthogonal to each other, making it hard for the user to keep them all in their head at once. -->
  <!-- Random projections alleviate this issue but it is hard to link between many random projections and assure the quality of each of them. -->
  So instead of PCA, we visualize this equal variance data by choosing random projections, using a technique called the Grand Tour<d-cite key="asimov1985grand"></d-cite>.
  The Grand Tour is a linear method which generates infinitely many projections and interpolate them smoothly.
</p>

<d-figure class='nngt-single-epoch-0' style="">
  <canvas id='nngt-single-epoch-0' class='grandtour'></canvas>
  <script src='js/se0.js'></script>
</d-figure>
<p class='caption'>The Grand Tour of the softmax layer. Drag the handle on each axis to move that axis.</p>


<p>
  When combined with interactive components such as sliders, data filters and direct manipulations, we can quickly find projections that reveals important patterns in the data. 
  In the above view we can click on legends, drag sliders and move the circular handle on each axis to (re)discover the late learning of digit 1 or 7 in MNIST, or the three-way confusion in Fashion-MNIST.
  But see how much easier this time than previous non-linear methods.
</p>
<p>
  Direct manipulation handles are not only for <em>exploratory</em> visual analysis, they are also friendly for <em>explanatory</em> visualizations/presentations. 
  To illustrate the late learning of digit 1 and 7 in the MNIST classifier, for example, we find and recorded the following linear projection which project the first and and seventh axis on top of the canvas. 
  Without much difficulty, we can convince the viewers that digit 1 and 7 get classified correctly starting from epoch 14 and 21, thanks to the good data-visual correspondence in the Grand Tour/linear projections. 
</p>

<d-figure class='nngt' style="">
  <canvas id='nngt' class='grandtour'></canvas>
  <script src='js/nngt.js'></script>
</d-figure>
<p class='caption'>
  From
  <a onclick="nngt.gt.setMatrix(DIGIT17_MATRIX)">this linear projection</a>, we can easily identify the learning of 
  <span style="color: #ff7f0e;">digit 1</span> on 
  <a onclick="nngt.setEpochIndex(13)">epoch 14</a> and <br>
  <span style="color: #7f7f7f;">digit 7</span> on 
  <a onclick="nngt.setEpochIndex(19)">epoch 21</a>.
</p>

<p>
  Now, let us take a closer look at the Grand Tour technique itself.
</p>


<!-- <p>
  Although with the Grand Tour we have observed interesting phenomenon, there are other ways of looking at the same kind of data. 
  For example, the
  <d-cite key="ren2017squares">Squares</d-cite>
  visualizes the softmax layer using parallel coordinates. 
  However, one draw back of using parallel coordinates is that it does not emphasize the spatial relation among multiple dimensions. 
  What makes it not ideal, is that the order of different dimension is predetermined so that any non-neighboring dimensions are hard to compare and correlate. 
  In our use case, where relation among dimensions are of the greatest interest, parallel coordinates would not provide an straightforward view of multi-way confusions among classes. 
</p> -->


<!-- this fails at providing an overview among all instances-->
<!-- <p>Similar illustrations exist, for example, by <d-cite key="harley2015isvc">Harley</d-cite>, where activations are arranged in a 2D or 3D canvas</p> -->
<!-- <p>
Mathematically, a neural network can be represented as a function. 
To make a neural network well modularized and easy to build and train, internally the mathematical function is composed by multiple simpler building-block functions, which are often called layers in neural network context. Such building-block functions serve different purposes. 
The labels on each colored rectangle denote what kind of building-block functions were used in our network. 
When an example image were passed through the network, each building-block function transforms the image to a different numerical representation and as the functions works in a chain, the image is transformed from one representation to another. 
The representations can be in the form of a scale, vector or tensor. 
In the diagram above, we illustrated some of these representations by gray-scale heatmaps. The whiter a pixel is, the larger value stored in the corresponding entry. To better use the drawing space, we reshaped these representation closer to squares.
A worth noting representation is the 10-vector produced in the last <em>softmax</em>layer. 
In this layer, the networks is trained to represent an one-hot encoding of the image class label, so normally only the entry corresponding to the true class of the image are fired up. 
For example, a digit 0 should have a white spot in the first column, first row. 
A digit 1 image should fire up the cell in first column, second row.
</p>
<p>
  One nice property of the softmax layer is that each entries are non-negative and the sum of them equals to 1. This property gives us an attractive interpretation of this layer - for any given image, the final softmax layer represents the <em>confidence</em> of the network about this image belonging to each class. In stead of looking at one image at a time, we would like to see their <em>distribution</em> in this nice interpretable softmax layer. 
  To see how this 10-vector is activated by the <em>set</em> of images, we turned to an old technique called the Grand Tour to reveal spatial relations among dimensions.
</p> -->

<h3>The Grand Tour</h3>
<p>
The Grand Tour<d-cite key="asimov1985grand"></d-cite> is a technique for high dimensional data visualization. 
Starting with a random velocity, it smoothly rotates data points around the origin in high dimensional space, and then project it down to 2D for display. 
Here are some examples of how Grand Tour acts on some (low-dimensional) objects:
</p>
<ul>
<li>On a square, the Grand Tour rotates it with a constant angular velocity.</li> 
<li>On a cube, the Grand Tour rotates it in 3D, and its 2D projection let us see every facet of the cube. </li>
<li>On a 4D cube (a <em>tesseract</em>), the rotation happens in 4D and the 2D view shows every possible projections.</li>
</ul>
<d-figure class='tesseract' style="grid-column: text;">
<canvas id='tesseract' style='width: 100%; height: 300px;'></canvas>
<p class='caption'>Grand tours of a square, a cube and a tesseract</p>
<script src='js/tesseract.js'></script>
</d-figure>

<!-- <p>
  Mathematically, the Grand Tour provides us a sequence of <d-math>p-dimensional</d-math> to 2D projections, which can be represented by a function from a time variable <d-math>t</d-math> to p-by-2 matrices. 
  The p-by-2 matrices can be seen as a composition of a p-by-p orthogonal matrix, <d-math>GT \in SO(p)</d-math>, followed by a simple projection <d-math>\pi_2</d-math> that picks up the first two coordinates of p-dimensional vectors:
  <d-math block>
    t \mapsto \pi_2 \circ GT 
  </d-math>
  We will need the above detail for direct manipulation in <a href="#technical-details">technical details</a>.
</p> -->
<p>
  For more details about the Grand Tour, we recommend curious readers to consult the original paper by Asimov<d-cite key="asimov1985grand"></d-cite>.
</p>


<p>
  We have to emphasize one favorable property of the Grand Tour in visualizing neural networks: 
  it is invariant about the (orthonormal) basis chosen for the data representation. 
  Because the Grand Tour itself is a change of basis, every solid rotation in data space has a corresponding Grand Tour matrix that result in the same view. 
  In the mean time, linear transformations in neural networks are also invariant under change of basis. 
  Every linear transformations can be uniquely decomposed as a three-step action: rotation, projection followed by another rotation<d-cite key="austin2009svd"></d-cite>.
  We will later show that this connection suggests a way to simplify all linear transformations in the neural networks when viewing it in the Grand Tour. 
</p>


<h2>The Grand Tour of the Softmax Layer</h2>
<p>
  We first look at the Grand Tour of the softmax layer. 
  The softmax layer is relatively easy to understand because its axes have strong semantics - the <d-math>i^{th}</d-math> axis corresponds to network's <em>confidence</em> on the <d-math>i^{th}</d-math> class label. 
  With this nice semantics, direct manipulations with axis handles come more handy.
  For other intermediate layers, the axes are more abstract so we deployed another technique which we will see shortly.
</p>

<i id="nngt-single-epoch-a"></i>

<d-figure class='nngt-single-epoch-1' style="">
  <canvas id='nngt-single-epoch-1' class='grandtour'></canvas>
  <script src='js/se1.js'></script>
</d-figure>
<p class='caption'>
  The Grand Tour of softmax layer in the last (99<sup>th</sup>) epoch, with 
  <a onclick="utils.setDataset('mnist');">MNIST</a>, 
  <a onclick="utils.setDataset('fashion-mnist');">fashion-MNIST</a> or 
  <a onclick="utils.setDataset('cifar10');">CIFAR-10</a> dataset.
</p>

<p>
  First, the Grand Tour of the softmax layer let us qualitatively judge the performance of our model.
  In addition, since we used similar architecture on three datasets, the performance also reflects the relative complexity of the dataset. 
  We can see that data points are most clustered for the MNIST dataset, the digits are close to one of the ten corners of the softmax space, while fashion-MNIST or CIFAR-10, the separation were not as extreme and more points live <em>inside</em> the volume.
  <!-- <strong><br><br>[TODO Figure showing all three datasets side-by-side]<br><br></strong> -->
</p>

<h3>Direct Manipulation on the Axes</h3>

<p>
  In fashion-MNIST, we see how the model confuses between classes.
  For example, the model confuses among sandals, sneakers and ankle boots, as we see data points form a triangular frame in the softmax layer.
  In the figure below, drag any handle on the axes to change the projection:
</p>

<d-figure class='nngt-single-epoch-2' style="">
  <canvas id='nngt-single-epoch-2' class='grandtour'></canvas>
  <script src='js/se2.js'></script>
</d-figure>
<p class='caption'>
  This <a onclick="
    se2.gt.setMatrix(SSA_MATRIX);
    se2.overlay.onSelectLegend(d3.range(10));
    se2.overlay.selectedClasses = new Set();
  ">linear projection</a> clearly shows model's confusion among
  <span style="color: #8c564b;">sandals</span>,
  <span style="color: #7f7f7f;">sneakers</span>, and
  <span style="color: #17becf;">ankle boots</span>.
  Similarly, <a onclick="
    se2.gt.setMatrix(PCS_MATRIX);
    se2.overlay.onSelectLegend(d3.range(10));
    se2.overlay.selectedClasses = new Set();
  ">this projection</a> shows the true three-way confusion about
  <span style="color: #2ca02c;">pullovers</span>,
  <span style="color: #9467bd;">coats</span>, and
  <span style="color: #e377c2;">shirts</span>.
  (The <span style="color: #e377c2;">shirts</span> are also get confused with 
  <span style="color: #1f77b4;">t-shirts/tops</span>. )
  Both projections are found by direct manipulations.
  <br>
  
</p>

<p>
  Examples that fall between sandal and sneaker classes indicate that the model has low confidence in distinguishing the two, same thing happens between sneaker and ankle boot classes. 
  But the model has less confusion between sandal and ankle boot classes, as not many examples fall between these two classes. 
  Moreover, all but one example falls in the interior of the triangle, while others live close to the boundary. 
  This tells us that most confusions happen between two out of the three classes, they are really two-way confusions.

  Within the same dataset, we can also see pullovers, coats and shirts filling a triangular <em>plane</em>.
  This is different from the sandal-sneaker-ankle-boot case, as examples not only fall on the boundary of a triangle, but also in its interior: a true three-way confusion. 

  Similarly, in the CIFAR-10 dataset we can see confusion between dogs and cats, airplanes and ships.
  The mixing pattern in CIFAR-10 is not as clear as in fashion-MNIST, because a lot of examples are misclassified.
</p>


<d-figure class='nngt-single-epoch-3' style="">
  <canvas id='nngt-single-epoch-3' class='grandtour'></canvas>
  <script src='js/se3.js'></script>
</d-figure>
<p class='caption'>
  This <a onclick="
    se3.gt.setMatrix(CD_MATRIX);
    se3.overlay.onSelectLegend(d3.range(10));
    se3.overlay.selectedClasses = new Set();
  ">linear projection</a> clearly shows model's confusion between
  <span style="color: #d62728; ">cats</span> and
  <span style="color: #8c564b;">dogs</span>.
  Similarly, <a onclick="
    se3.gt.setMatrix(AS_MATRIX);
    se3.overlay.onSelectLegend(d3.range(10));
    se3.overlay.selectedClasses = new Set();
  ">this projection</a> shows the confusion about
  <span style="color: #1f77b4;">airplanes</span> and
  <span style="color: #bcbd22;">ships</span>.
  Both projections are found by direct manipulations.
</p>
<p>
  We will explain how direct manipulation works in <a href="#technical-details">technical details</a>.
</p>


<h2>The Grand Tour of Training Dynamics</h2>

<p>
  We trained 99 epochs and recorded the entire history of neuron activations on subsets of training and testing examples. 
  In the beginning when neural network first randomly initialized, all examples are placed around the origin of softmax space. 
  Through training, examples are pushed to their correct "class corners" - the one hot vectors - in the softmax space.
</p>

  <!-- In softmax layer, our neural networks aim to match each image to its 'one-hot' encoding of the corresponding class label. 
  Naturally, if we treat the one-hot encoding as a 10-dimensional vector, in the ideal case we want the neural network cluster instances of <d-math>i^{th}</d-math> class close to coordinate <d-math>e_i = (0,0,\cdots, 0, 1, 0, \cdots)</d-math>, with its <d-math>i^{th}</d-math> coordinate being 1 and 0 elsewhere. 
  As a property of the softmax function we used, no coordinates exceed 1 and all the 10 coordiantes of an instance sum up to 1, so after the softmax function, our points live in a <d-math>10</d-math>-simplex.
  Geometrically this means that we expect a well-trained network map images of one specific class to the corresponding corner of the <d-math>10</d-math>-simplex, as we have mentioned in the teaser figure. 
  We repeated the same figure as below. 
  To interact, you can switch to different datasets from the top menu in the navigation bar, 
  filter images by its class by hovering over and clicking the legend. 
  Dragging the 10 bubble handles at every tip of the ten axes will intervene the rotation. 
  When the Grand Tour glides to or you manually find a good view, it is good to pause the rotation with the button provided on top left and look at how the network trained from that fixed perspective. 
  We also provide a switch to look at the actual images instead of arbitrary points. 
  The image view is best accompanied with an expanded canvas.  -->

<p>
  Comparing the difference between training and testing datasets gives us an estimation about over-fitting. 
  In the MNIST dataset, the trajectory of testing images through training is consistent with the training set. 
  Data points went directly toward the corner of its true class and all classes are stabilized after about 50 epochs.
  On the other hand, in CIFAR-10 there is an <em>inconsistency</em> between the training and testing sets. Images from the testing set keep oscillating while most images from training converges to the corresponding class corner. 
  In epoch 99, we can clearly see a difference in distribution between these two sets.
  This signals that the model overfits the training set and thus does not generalize well to the testing set. 
</p>

<d-figure class='nngt2' style="">
  <canvas id='nngt2' class='grandtour'></canvas>
  <script src='js/nngt2.js'></script>
</d-figure>
<p class='caption'>
  With <a onclick="
    utils.setDataset('cifar10'); 
    nngt2.gt.setMatrix(TT_MATRIX); 
    nngt2.setEpochIndex(99);
    nngt2.shouldAutoNextEpoch = true; //set to true then click play button to pause it.
    nngt2.overlay.playButton.on('click')();
    ">
  this view of CIFAR-10</a> , 
  the color of points are more mixed in testing (right) than training (left) set, showing an over-fitting in the training process.
  Also see this comparison in 
  <a onclick="utils.setDataset('mnist');">MNIST</a>
  or <a onclick="utils.setDataset('fashion-mnist');">fashion-MNIST</a>, 
  where there is less difference between training and testing sets.
</p>





<h2 name="layer-dynamics">The Grand Tour of layer dynamics</h2>
<!-- <p>
  Up till now we have seen the last softmax layer of out neural network model. 
  But what happens to other intermediate layers? Can we look at them in the similar way we did for the last layer? One challenge of looking at intermediate layers, it that axis no longer have a semantic meaning, as in the last layer we know that each of the 10 directions correspond to one of the ten label classes. Another challenge is that directions in two different layers do not align with each other by default. Thus making sense of how the model transforms data points between two consecutive layers is non-trivial.
</p>  --> 

<p>
  Given the presented techniques of the Grand Tour and direct manipulations on the axes, we are in theory able to visualize and manipulate any intermediate layer of a neural network. 
  Despite that, two problems need to be solved first:
  <ul>
    <li>
      While we want to trace out the activation patterns through different layers, the points going under a linear transformation can be hard to trace in Grand Tour.
      Under linear transformations, the dimensionality might change, and each neuron in a later layer is resulted from mixing activation of all neurons in the previous layer. 
    </li>
    <li>
      The handles on axes of softmax layer were very convenient, but it is only practical when dimensionality is small.
      With hundreds of dimensions, for example, the axis handles would look like a hairball, making them impossible in interact with.
      Moreover, hidden layers do not have as clear semantics as the softmax layer, so manipulating axes in hidden layers would not be as natural and reasonable. 
    </li>
  </ul>
</p>

<p>
  To address the first problem, we simplify the linear transformations by observing that they are equivalent to simple scalings under the view of the Grand Tour.
  To see how a linear transformation can be viewed unnecessarily complicated, think about a neural network layer <d-math>A</d-math> that negates two axis in 2D, in matrix form:
  <d-math block>
    A = \begin{bmatrix}
    -1, 0 \\
    0, -1
    \end{bmatrix}
  </d-math>
  One way to interpolate the source <d-math>x_0</d-math> and destination <d-math>x_1 = A(x_0) = -x_0</d-math> of this action <d-math>A</d-math> is by a simple linear interpolation

  <d-math block>
  x_t = (1-t) \cdot x_0 + t \cdot x_1 = (1-2t) \cdot x_0    
  </d-math>
  for <d-math>t \in [0,1].</d-math>

  If we apply this interpolation on all data points, they will cross the origin at the same time (<d-math>t=0.5</d-math>), making it impossible trace individual points. 
  Moreover, this interpolation complicates a simple 180 degree rotation in data.
  In the Grand Tour, matrix <d-math>A</d-math> should be considered doing nearly nothing, for it only rotates the data while the Grand Tour is also a rotation.
  However, when visualizing this by the simple linear interpolation above, all points move dramatically, while the distribution of points (in a coordinate-free space) stays the same.
  Therefore, this interpolation fails to meet the principle of data-visual correspondence: a simple change in data (negation in 2D/ 180 degree rotation) results in a drastic change in visualization (all points cross the origin).
</p>
<p>
  When designing a visualization, we should think about which parts of the data are artifacts that we should discard, and which contain the real knowledge we should <em>distill</em> from this representation. 
  In this case, the rotational factors in linear transformations are not as important other factors such as scalings.
  So by design, we use the Grand Tour because it is invariant to rotations in data.
  In turn, the rotational components in our data (i.e. rotations in linear transformations), whenever complicate the visualization unnecessarily, should be explicitly discarded.
</p>
<p>
  To see how it works in reality, we take advantage of a central theorem of linear algebra. 
  The <em>Singular Value Decomposition</em> (SVD) theorem shows that <em>any</em> linear transformation can be decomposed into a sequence of very simple operations: a rotation, a scaling, and another rotation<d-cite key="austin2009svd"></d-cite>. 

  <!-- (From another perspective, any linear operation in standard bases is a scaling operation with appropriately chosen orthonormal bases on both sides.  -->
  <!-- We will use this perspective later when we discuss what part of the data representation is discarded by the Grand Tour. ) -->
  Applying a matrix <d-math>A</d-math> to a vector <d-math>x</d-math> is then equivalent to applying those simple operations: <d-math>x A = x U \Sigma V^T</d-math>.
  But remember that the Grand Tour works by rotating the dataset and then projecting it to 2D.
  Combined, these two facts mean that as far as the Grand Tour is concerned, visualizing a vector <d-math>x</d-math> is the same as visualizing <d-math>x U</d-math>, and visualizing a vector <d-math>x U \Sigma V^T</d-math> is the same as visualizing <d-math>x U \Sigma</d-math>. 
  This means that any linear transformation seen by the Grand Tour is equivalent to the transition between <d-math>x U</d-math> and <d-math>x U \Sigma</d-math> - a simple (coordinate-wise) scaling. 
  This is explicitly saying that any linear operation (whose matrix is represented in standard bases) is a scaling operation with appropriately chosen orthonormal bases on both sides.
  So the Grand Tour provides a natural, elegant and computationally efficient way to <em>align</em> visualizations of activations separated by a fully-connected (linear) layer.
  Note that for convolutional layers, they can also be seen as linear transformations between flattened feature maps, so the same aligning mechanism applies to convolutional layers as well.
</p>

<p>
  Since we have activations of all layers, to reduce the size of data, we cut the number of data points to 500 and epochs to 50.
  Despite less points and epochs are shown, now we are able to trace a behavior we observed in softmax back to any layers.
  In fashion-MNIST, for example, we observe a separation of shoes (sandals, sneakers and ankle boots as a group) from all other classes in the softmax layer. 
  Tracing it back to earlier layers, we can see that this separation happened as early as <a onclick="lt2Figure.onscreen()">layer 5</a>:
</p>
<d-figure class='lt2' style="">
  <canvas id='lt2' class='layertransition'></canvas>
  <script src='js/lt2.js'></script>
</d-figure>
<p class='caption'>With layers aligned, it is easy to see the early separation of shoes from <a onclick="
lt2.onDatasetChange('fashion-mnist');
lt2Figure.onscreen();
lt2.overlay.layerPlayButton.on('click')();
">this view</a>.</p>

<p>We found this early separation with manipulations <em>directly on data points</em>, which we will explain right now.</p>

<h3>From Basis to Instances: Extending Direct Manipulations</h3>
<p>
  We tackle the second problem by allowing users to directly manipulate a subset of <em>points</em>.
</p>
<p> 
  Unlike the softmax layer, there is one problem with axis handles in hidden layers.
  Even though we preprocessed hidden layer activations (we chose up to 45 principle components through PCA) so that their dimensionality is at a manageable size for the Grand Tour, the axes lines can still make a hairball, and more importantly, these axes in hidden layers are not as meaningful as in the softmax case. 
  <!-- Unlike the softmax (or pre-softmax) where each axis represents a label class, in hidden layers, one may need some indirect ways (e.g. feature visualization <d-cite key="olah2017feature"></d-cite>) to understand the semantics. -->
  <!-- But even we used some feature visualization techniques, these derived meanings are hard to remember or encode in the Grand Tour.  -->
  Thus we should not have axis handles anymore.
  Instead, we can regard <em>data points</em> as handles this time. 
  In the figure above, we can brush a subset of data point, an orange circle at their centroid will show up and we use it as a handle to change the projection. 
  It is worth emphasizing that manipulations on data points are more general, because axis handles can be regarded as special data points as well.
  Moreover, they are applicable in even higher dimensions where axis handles are too many and/or have less semantics.
  We will cover how it works in <a href="#technical-details">technical details</a>.
</p>
<p>
  To summarize, by observing the invariance in Grand Tour, we simplified linear transformations in neural networks by removing their rotational components.
  To tackle the hairball effect when the dimensionality is large, we removed axis handles and directly manipulation points instead.
  We want to emphasize again that when axes have a clear semantics and there is only a few of them, like in the softmax layer of a 10-class dataset, we should use the axes. 
  In other cases, we should directly manipulate data points because either the axes may not be as interpretable as the softmax (e.g. intermediate layers), and/or there can be too many dimensions which has a hairball effect (e.g. a softmax layer of a 1000-class classification problem). 
</p>


<h2>The Grand Tour of Adversarial Dynamics</h2>
<p>
  The Grand Tour can also elucidate adversarial examples <d-cite key="szegedy2013intriguing"></d-cite> as they are processed by a neural network.
  For this illustration, we adversarially add perturbations to 89 digit 8s to fool the network into thinking they are 0s.
  In the previous sections, we animate either the training dynamics or the layer dynamics. 
  Here, we fix a well-trained neural network and a layer of interest, and visualize the evolution of adversarial examples, for which we used the Fast Gradient Sign method <d-cite key="goodfellow2014explaining"></d-cite>at each step. 
  Again, because the Grand Tour is a linear method, the change in the positions of the adversarial examples over time can be faithfully attributed to changes in how the neural network perceives the images, rather than potential artifacts of the visualization.
</p>
<p>
  Let us examine how adversarial examples evolved to fool the network:
</p>
<d-figure class='lta' style="">
  <canvas id='lta' class='layertransition'></canvas>
  <script src='js/lta.js'></script>
</d-figure>
<p class="caption">
  From <a onclick="ltaFigure.onscreen()">this view of softmax</a>, we can see how  
  <span style="color: #444444">adversarial examples</span> 
  evolved from <span style="color: #bcbd22">8s</span> 
  into <span style="color: #1f77b4">0s</span>.
  In the corresponding <a onclick="ltaFigure.onscreen(); lta.overlay.onLayerSliderInput(10);">pre-softmax</a> however, these adversarial examples stop around the decision boundary of two classes. 
  Show data as <a onclick="lta.setMode('image')">images</a> to see the actual images generated in each step, or <a onclick="lta.setMode('point')">dots</a> colored by labels.
</p>

<p>
  We start from 89 examples of digit 8 (black dots in the plot above) and gradually add adversarial noise to these images, using the fast gradient sign method <d-cite key="goodfellow2014explaining"></d-cite>.
  Through this adversarial training, the network eventually beliefs that all these images are 0s. 
  If we stay in the softmax layer and slide though the adversarial training steps in the plot, we can see adversarial examples move from a high score for class 8 to a high score for class 0.
  Although all adversarial examples are classified as the target class (digit 0s) eventually, some of them detoured somewhere close to the origin (around the 25th epoch) and then moved towards the target. 
  Comparing the actual images of the two groups, we see those that the detoured images are noisier.
</p>
<p>
  What is more interesting is what happens in the intermediate layers. 
  In pre-softmax, for example, we see that these <span style="color: #444444; font-weight: 550;">fake 0s</span> behave differently from the <span style="color: #1f77b4; font-weight: 550;">genuine 0s</span>: they live closer to the decision boundary of two classes and form a plane by themselves. 
</p>

<h2>Discussion</h2>

<h3>The Power of Animation and Direct Manipulation</h3>
<p>
  When comparing linear projections with non-linear dimensionality reductions, we used small multiples that go over different training epochs and dimensionality reduction methods. 
  Later when we used the Grand Tour alone, however, we used a single view with animations.
  When comparing small multiples and animations, there is no general conclusion on which one is better than the other in the literature. 
  They are only studied in specific settings, for example, in dynamic graph drawing <d-cite key="archambault2010animation"></d-cite>.
  There has been concerns about the incomparable contents <d-cite key="tversky2002animation"></d-cite> between small multiples and animated plots in some of these studies.
  Despite these concerns, however, the use of animation comes naturally from the direct manipulation in our case.
  Direct manipulation and animation give us opportunities to see, one at a time, all possible projections of the data within a single view, whereas with small multiples we can only select a small sample among infinitely possible projections. 
  Although we could have used small multiples rather than animations for training and layer dynamics, without specific studies supporting that small multiples are significantly better, we choose to corporate all features in a single animated view to keep our visualization minimal and focused.
</p>


<h3>Non-sequential Models</h3>
<p>
  In our work we have walked through models that are purely sequential. 
  In modern architectures it is common to have non-sequential parts such as highway <d-cite key="srivastava2015highway"></d-cite> branches or dedicated branches for different tasks <d-cite key="szegedy2015going"></d-cite>. 
  With our technique, one can visualize neuron activations on each such branch, but more interesting to us in the future is how to visually incorporate multiple branches and interpret much more complex architectures.
</p>


<h3>Scaling to Larger Models</h3>
<p>
  Modern architectures are also wide. Especially when convolutional layers are concerned, one could run into issues with scalability if we see such layers as a large sparse matrix acting on flattened multi-channel images.
  For the sake of simplicity, in this article we brute-forced the computation of the alignment of such convolutional layers by writing out their explicit matrix representation. 
  However, the singular value decomposition of multi-channel 2D convolutions can be computed efficiently <d-cite key="sedghi2018singular"></d-cite>, which can be then be directly used for alignment, as we described above.
</p>


<h2>Conclusion</h2>

<p>
  Although the Grand Tour is not a perfect solution for high-dimensional data visualization, the examples above highlight the importance of ensuring that the properties of the phenomenon we want to visualize are in fact respected by the visualization method. As powerful as t-SNE and UMAP are, they often fail to offer the correspondences we need, and such correspondences can come, surprisingly, from relatively simple methods like the ones we presented.
</p>
<p>
  In the interest of completeness, the remainder of the article provides a number of technical details about the implementation of the techniques we described above.
</p>



<script>
function toggle(event, id){
  let caller = d3.select(event.target); //DOM that received the event
  let callerIsActive = caller.classed('clickable-active');

  let selection = d3.select(id); //DOM to be toggled
  let isHidden = selection.classed('hidden');

  selection.classed('hidden', !isHidden);
  caller.classed('clickable-active', !callerIsActive); //change the indicator (+/- sign) besides the caller
}
</script>


<!-- <h2 class="clickable clickable-active" onclick="toggle(event, '#div-technical-details')"><a name="technical-details"></a>
Technical Details
</h2>
<div id='div-technical-details' class=''> -->

<h2 class="clickable" onclick="toggle(event, '#div-technical-details')"><a name="technical-details"></a>
Technical Details
</h2>
<div id='div-technical-details' class='hidden'>

In this sections, we will introduce the technical details of direct manipulation and layer transition.

<h3>
Notations
</h3>

<p>
  In this section, our notational convention is that data are represented as row vectors. 
  They form a matrix in which each row is a record and each column is a different feature/dimension.
  As a result, when a linear transformation is applied to the data, the transformation matrix is multiplied on the right of the data matrix.
  This has a side benefit that when applying matrix multiplications in a chain, the formula reads from left to right and aligns with a commutative diagram.
  For example, when a data matrix <d-math>X</d-math> is multiplied by a matrix <d-math>M</d-math> to generate <d-math>Y</d-math>, in formula we write <d-math>XM = Y</d-math>, the letters have the same order in diagram:
</p>

<d-math block>
    X 
    \overset{M}{\mapsto}
    Y
</d-math>

Further, if the SVD of <d-math>M</d-math> is <d-math>M = U \Sigma V^{T}</d-math>, we have <d-math>X U \Sigma V^{T} = Y</d-math>, while the diagram
<d-math block>
    X 
    \overset{U}{\mapsto} <!-- XU  -->
    \overset{\Sigma}{\mapsto} <!-- XU\Sigma  -->
    \overset{V^T}{\mapsto} Y
</d-math>
is nicely aligned with the formula.

<h3 class="clickable clickable-active" onclick="toggle(event, '#div-direct-manipulation')">
Direct manipulation
</h3>
<div id='div-direct-manipulation' class=''>

<p>
  Our direct manipulations provide direct control over possible projections of data points.
  It can be done either on the axes or on a group of data points. 
  Based on the dimensionality and axis semantics, as discussed in <a href='#layer-dynamics'>Layer Dynamics</a>, we may prefer one mode than the other.
  

  The goal of direct manipulations is to change the linear projection according to user's demand, which is provided by for example mouse dragging. 
  We will see that the axis mode is a special case of data point mode, because we can view an axis handle as a special point on the positive direction of the axis. 
  But because of its simplicity, we will first introduce axis mode.
</p>

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-axis-mode')">
  The Axis Mode
</h4>
<div id='div-axis-mode' class=''>

<p>
  When a user drags an axis handle, he/she wants to move this vector in that direction.
  Since the projection is done by the Grand Tour, the goal is to find a new Grand Tour projection so that the axis follows the user's direction.

  In a nutshell, when user drags the <d-math>i^{th}</d-math> axis handle by <d-math>(dx, dy)</d-math>, we add them to the first two entries of the <d-math>i^{th}</d-math> row of the Grand Tour matrix, and do <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt orthonormalization</a> on the rows of the new matrix
  <d-footnote>
    Rows have to be reordered such that the <d-math>i^{th}</d-math> row is considered first in the Gram-Schmidt.
  </d-footnote>.
</p>

<p>
  To see why, first, let us formalize the process of the Grand Tour on a standard basis vector <d-math>e_i</d-math>. 
  As shown in the diagram below, <d-math>e_i</d-math> goes through an orthogonal Grand Tour matrix <d-math>GT</d-math> to produce a rotated version of itself, <d-math>\tilde{e_i}</d-math>. 
  Then, <d-math>\pi_2</d-math> is a function that keeps only the first two entries of <d-math>\tilde{e_i}</d-math> and gives the 2D coordinate of the handle to be shown in the plot, <d-math>(x_i, y_i)</d-math>.
</p>

<!-- <img 
src="figs/direct-manipulation-axis-mode-commutative-diagram-0.png" 
class="img-center"
style="width: 40%"
/> -->
<d-math block>
  e_i \overset{GT}{\mapsto} \tilde{e_i} \overset{\pi_2}{\mapsto} (x_i, y_i)
</d-math>
<p>
  When user drags an axis handle on canvas, he/she induces a delta change <d-math>\Delta = (dx, dy)</d-math>on the xy-plane. 
  The coordinate of the handle becomes:
  <d-math block>(x_i^{(new)}, y_i^{(new)}) := (x_i+dx, y_i+dy)</d-math>
  Note that <d-math>(x_i, y_i)</d-math> is the first two coordinates of the axis handle in high dimension after the Grand Tour rotation, so a delta change on <d-math>(x_i, y_i)</d-math> induces a delta change <d-math>\tilde{\Delta} := (dx, dy, 0, 0, \cdots)</d-math> on <d-math>\tilde{e_i}</d-math>:
  <d-math block>\tilde{e_i} \overset{\tilde{\Delta}}{\mapsto} \tilde{e_i} + \tilde{\Delta}</d-math>
</p>
<p>
  To find a nearby Grand Tour rotation that respects this change, first note that <d-math>\tilde{e_i}</d-math> is exactly the <d-math>i^{th}</d-math> row of orthogonal Grand Tour matrix <d-math>GT</d-math> 
  <d-footnote>
    Recall that the convention is that vectors are in row forms and linear transformations are matrices that are multiplied on the right.
    So <d-math>e_i</d-math> is a row vector whose <d-math>i^{th}</d-math> is 1 and 0s elsewhere, and <d-math>\tilde{e_i} := e_i \cdot GT</d-math> is the <d-math>i^{th}</d-math> row of <d-math>GT</d-math>
  </d-footnote>. 
  Naturally, we want the new matrix to be the original <d-math>GT</d-math> with its <d-math>i^{th}</d-math> row replaced by <d-math>\tilde{e_i}+\tilde{\Delta}</d-math>, i.e. we should add <d-math>dx</d-math> and <d-math>dy</d-math> to the <d-math>(i,1)</d-math>-th entry and <d-math>(i,2)</d-math>-th entry of <d-math>GT</d-math> respectively:
  <d-math block>\tilde{GT} \leftarrow GT</d-math>
  <d-math block>\tilde{GT}_{i,1} \leftarrow GT_{i,1} + dx</d-math>
  <d-math block>\tilde{GT}_{i,2} \leftarrow GT_{i,2} + dy</d-math>
  However, <d-math>\tilde{GT}</d-math> is not orthogonal for arbitrary <d-math>(dx, dy)</d-math>.
  In order to find an approximation to <d-math>\tilde{GT}</d-math> that is orthogonal, we apply <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt orthonormalization</a> on the rows of <d-math>\tilde{GT}</d-math>, with the <d-math>i^{th}</d-math> row considered first in the Gram-Schmidt process:
  <d-math block>GT^{(new)} := GramSchmidt(\tilde{GT})</d-math>
  Note that the <d-math>i^{th}</d-math> row is normalized to a unit vector during the Gram-Schmidt, so the resulting position of the handle is 
  <d-math>\tilde{e_i}^{(new)} = normalize(\tilde{e_i} + \tilde{\Delta})</d-math>
  which may not be exactly the same as <d-math>\tilde{e_i}+\tilde{\Delta}</d-math>, as the following figure shows
  <d-footnote>
    However, for any <d-math>\tilde{\Delta}</d-math>, the norm of the difference is bounded above by <d-math>||\tilde{\Delta}||</d-math>, as the following figure proves.
    <img src="figs/direct-manipulation-rotation-2d-proof.png" style="width: 100%" class="img-center"/>
  </d-footnote>
  .
</p>
<img src="figs/direct-manipulation-rotation-2d.png" style="width: 40%" class="img-center"/>


</div> <!-- div-axis-mode -->

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-data-point-mode')">
  The Data Point Mode
</h4>
<div id='div-data-point-mode' class=''>
<p>
  We now explain how we directly manipulate data points. 
  Technically speaking, this method only considers one point at a time.
  For a group of points, we compute their centroid and directly manipulate this single point with this method.
  Thinking more carefully about the process in axis mode gives us a way to drag any single point.
  Recall that in axis mode, we added user's manipulation <d-math>\tilde{\Delta} := (dx, dy, 0, 0, \cdots)</d-math> to the position of the <d-math>i^{th}</d-math> axis handle <d-math>\tilde{e_i}</d-math>.
  This induces a delta change in the <d-math>i^{th}</d-math> row of the Grand Tour matrix <d-math>GT</d-math>.
  Next, as the first step in Gram-Schmidt, we normalized this row: 
  <d-math block>
    GT_i^{(new)} := normalize(\tilde{GT}_i) = normalize(\tilde{e_i} + \tilde{\Delta})
  </d-math>
  These two steps make the axis handle move from <d-math>\tilde{e_i}</d-math> to <d-math>\tilde{e_i}^{(new)} := normalize(\tilde{e_i}+\tilde{\Delta})</d-math>.
</p>
<p>
  Looking at the geometry of this movement, the "add-delta-then-normalize" on <d-math>\tilde{e_i}</d-math> is equivalent to a <em>rotation</em> from <d-math>\tilde{e_i}</d-math> towards <d-math>\tilde{e_i}^{(new)}</d-math>, illustrated in the figure below. 
  This geometric interpretation can be directly generalized to any arbitrary data point.
</p>
<img src="figs/direct-manipulation-rotation-3d.png" style="width: 65%" class="img-center"/>
<p>
  The figure shows the case in 3D, but in higher dimensional space it is essentially the same, since the two vectors <d-math>\tilde{e_i}</d-math> and <d-math>\tilde{e_i}+\tilde{\Delta}</d-math> only span a 2-subspace.
  Now we have a nice geometric intuition about direct manipulation: dragging a point induces a <em>simple rotation</em>
  <d-footnote><a href="https://en.wikipedia.org/wiki/Rotations_in_4-dimensional_Euclidean_space#Simple_rotations">Simple rotations</a> are rotations with only one <a href="https://en.wikipedia.org/wiki/Plane_of_rotation#Simple_rotations">plane of rotation</a>.</d-footnote>
  in high dimensional space.
  This intuition is precisely how we implemented our direct manipulation on arbitrary data points, which we will specify as below.
</p>
<p>
  Generalizing this observation from axis handle to arbitrary data point, we want to find the rotation that moves the centroid of a selected subset of data points <d-math>\tilde{c}</d-math> to 
  <d-math block>
    \tilde{c}^{(new)} := (\tilde{c} + \tilde{\Delta}) \cdot ||\tilde{c}|| / ||\tilde{c} + \tilde{\Delta}||
  </d-math>
</p>
<img src="figs/direct-manipulation-rotation-2d-perp.png" style="width: 40%" class="img-center"/>
<p>
  First, the angle of rotation can be found by their cosine similarity:
  <d-math block> \theta = arccos(
    \frac{
      \langle \tilde{c}, \tilde{c}^{(new)} \rangle
    }{
      ||\tilde{c}|| \cdot ||\tilde{c}^{(new)}||
    }
  )</d-math>
  Next, to find the matrix form of the rotation, we need a convenient basis.
  Let <d-math>Q</d-math> be a change of (orthonormal) basis matrix in which the first two rows form the 2-subspace <d-math>span(\tilde{c}, \tilde{c}^{(new)})</d-math>.
  For example, we can let its first row to be <d-math>normalize(\tilde{c})</d-math>, second row to be its orthonormal complement <d-math>normalize(\tilde{c}^{(new)}_{\perp})</d-math> in <d-math>span(\tilde{c}, \tilde{c}^{(new)})</d-math>, and the remaining rows complete the whole space:
  <d-math block>
  \tilde{c}^{(new)}_{\perp} 
  := \tilde{c} - ||\tilde{c}|| \cdot cos \theta \frac{\tilde{c}^{(new)}}{||\tilde{c}^{(new)}||}
  </d-math>

  <d-math block>
    Q :=
    \begin{bmatrix}
    \cdots normalize(\tilde{c}) \cdots \\
    \cdots normalize(\tilde{c}^{(new)}_{\perp}) \cdots \\
    P
    \end{bmatrix}
  </d-math>
  where <d-math>P</d-math> completes the remaining space.

  Making use of <d-math>Q</d-math>, we can find the matrix that rotates the plane <d-math>span(\tilde{c}, \tilde{c}^{(new)})</d-math> by the angle <d-math>\theta</d-math>:
  <d-math block>
    \rho = Q^T
    \begin{bmatrix}
    cos \theta& sin \theta&  0&  0& \cdots\\
    -sin \theta& cos \theta&  0&  0& \cdots\\
    0& 0&  \\ 
    \vdots& \vdots& & I& \\
    \end{bmatrix}
    Q
    =: Q^T R_{1,2}(\theta) Q
  </d-math>
  The new Grand Tour matrix is the matrix product of the original <d-math>GT</d-math> and <d-math>\rho</d-math>:
  <d-math>
    GT^{(new)} := GT \cdot \rho
  </d-math>
  Now we should be able to see the connection between axis mode and data point mode.
  In data point mode, finding <d-math>Q</d-math> can be done by Grand-Schmidt: Let the first basis be <d-math>\tilde{c}</d-math>, find the orthogonal component of <d-math>\tilde{c}^{(new)}</d-math> in <d-math>span(\tilde{c}, \tilde{c}^{(new)})</d-math>, repeatedly take a random vector, find its orthogonal component to the span of the current basis vectors and add it to the basis set. 
  In axis mode, the <d-math>i^{th}</d-math>-row-first Grand-Schmidt does the rotation and change of basis in one step.
</p>
</div> <!-- div-data-point-mode -->
</div> <!-- div-direct-manipulation -->


<h3 class="clickable clickable-active" onclick="toggle(event, '#div-align-representation')">
  <!-- Aligning representations: enabling the visualization of layer-to-layer dynamics -->
  Layer Transitions
</h3>
<div id='div-align-representation' class=''>

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-relu')">
  <d-math>ReLU</d-math> Layers
</h4>
<div id='div-relu' class=''>
  When the <d-math>l^{th}</d-math> layer is a ReLU function, the output activation is <d-math>X^{l} = ReLU(X^{l-1})</d-math>. Since ReLU does not change the dimensionality and the function is taken coordinate wise, we can animate the transition by a simple linear interpolation: for a time parameter <d-math>t \in [0,1]</d-math>,
  <d-math block>
    X^{(l-1) \to l}(t) := (1-t) X^{l-1} + t X^{l}
  </d-math>
</div> <!-- #div-relu -->

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-linear')">
  Linear Layers
</h4>
<div id='div-linear' class=''>
  Linear layers can seem complicated with a inappropriate basis. 
  For example, if <d-math>X^{l} = X^{l-1} M</d-math> where <d-math>M \in \mathbb{R}^{m \times n}</d-math> is the matrix of a linear transformation, then it has a singular value decomposition (SVD):
  <d-math block>M = U \Sigma V^T</d-math>
  where <d-math>U \in \mathbb{R}^{m \times m}</d-math> and <d-math>V^T \in \mathbb{R}^{n \times n}</d-math> are orthogonal, <d-math>\Sigma \in \mathbb{R}^{m \times n}</d-math> is diagonal.
  For arbitrary <d-math>U</d-math> and <d-math>V^T</d-math>, the transformation on <d-math>X^{l-1}</d-math> is a composition of a rotation (<d-math>U</d-math>), scaling (<d-math>\Sigma</d-math>) and another rotation (<d-math>V^T</d-math>), which can look complicated. 
  However, when thinking about these actions in Grand Tour which is rotations, we notice that <d-math>U</d-math> and <d-math>V^T</d-math> will have no significance in it - they are just another two rotations to the view.
  Hence, instead of showing <d-math>M</d-math>, we should animate <d-math>\Sigma</d-math> only.
  <d-math>\Sigma</d-math> is a coordinate-wise scaling, so we can animate it similar to the ReLU after the proper change of basis.
  Given <d-math>X^{l} = X^{l-1} U \Sigma V^T</d-math>, we have
  <d-math block>
    (X^{l}V) = (X^{l-1}U)\Sigma
  </d-math>
  For a time parameter <d-math>t \in [0,1]</d-math>,
  <d-math block>
    X^{(l-1) \to l}(t) := (1-t) (X^{l-1}U) + t (X^{l}V)
  </d-math>

</div> <!-- #div-linear -->

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-conv')">
  Convolutional Layers
</h4>
<div id='div-conv' class=''>
  Convolutional layers can be represented as special linear layers.
  With a change of representation, we can animate a convolutional layer like the previous section.
  For 2D convolutions this change of representation involves flattening the input and output, and repeating the kernel pattern in a sparse matrix <d-math>M \in \mathbb{R}^{m \times n}</d-math>, where <d-math>m</d-math> and <d-math>n</d-math> are the dimensionalities of the input and output respectively.
  This change of representation is only practical for a small dimensionality (e.g. up to 1000), since we need to solve SVD for linear layers.
  However, the singular value decomposition of multi-channel 2D convolutions can be computed efficiently <d-cite key="sedghi2018singular"></d-cite>, which can be then be directly used for alignment.
</div> <!-- #div-conv -->

<h4 class="clickable clickable-active" onclick="toggle(event, '#div-maxpool')">
  Max-pooling Layers
</h4>
<div id='div-maxpool' class=''>
  Animating max-pooling layers can be tricky since its not linear <d-footnote>A max-pooling layer is piece-wise linear</d-footnote> or coordinate-wise.
  We approximate it by average-pooling. 
  Similar to convolutional layers, we find the matrix form of average-pooling and use its SVD to align the view before and after this layer. 
</div> <!-- #div-maxpool -->
</div> <!-- #div-align-representation -->

</div><!-- #div-technical-details -->



</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    The utility code for WebGL under js/lib/webgl_utils/ are adapted from Angel's computer graphics book supplementary: 
    <a href="https://www.cs.unm.edu/~angel/BOOK/INTERACTIVE_COMPUTER_GRAPHICS/SEVENTH_EDITION/">https://www.cs.unm.edu/~angel/BOOK/INTERACTIVE_COMPUTER_GRAPHICS/SEVENTH_EDITION/</a>
  </p>


  <!-- <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p> -->


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
